{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1436e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# TensorBoard support\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_AVAILABLE = False\n",
    "    SummaryWriter = None\n",
    "    print(\"TensorBoard not available. Scalar logging will be disabled.\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped  # same as your TF code\n",
    "\n",
    "# Seeding for reproducibility (handles both old & new gym APIs)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "try:\n",
    "    env.reset(seed=seed)\n",
    "except TypeError:\n",
    "    # Older gym versions:\n",
    "    env.seed(seed)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "max_episodes_pg = 300           # vanilla policy gradient\n",
    "max_episodes_pg_baseline = 300  # policy gradient with baseline\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95  # Discount rate\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3ca7b",
   "metadata": {},
   "source": [
    "**1** : \n",
    "Explain why we need to treat the output of reset() and step() differently across Gym versions.\n",
    "\n",
    "Coding task: Fill in the TODOs so that reset_env and step_env work for both older and newer versions of Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81139c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def reset_env(env):\n",
    "    result = env.reset()\n",
    "    # TODO: handle both tuple and non-tuple returns\n",
    "    return state\n",
    "\n",
    "def step_env(env, action):\n",
    "    result = env.step(action)\n",
    "    # TODO: handle both old 4-return and new 5-return format\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc592fb8",
   "metadata": {},
   "source": [
    "**Q1.1**: (short coding)\n",
    "Fill in the missing line to select a random action:\n",
    "\n",
    "Replace the TODO with a line that samples uniformly from the environment’s action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d1645",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_random_agent(env, num_episodes=10, render=False):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"Random agent - EPISODE\", episode)\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # TODO (Q1.1): select an action using a *random guess*\n",
    "            # action = ???\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(\"Score:\", total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(\"Random agent average reward over\", num_episodes, \"episodes:\", avg_reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f56c4",
   "metadata": {},
   "source": [
    "**Q1.2**: (conceptual: “random guess” questions)\n",
    "\n",
    "CartPole episodes terminate when the pole falls or when the time limit (200 steps) is reached.\n",
    "\n",
    "What is the maximum possible reward per episode?\n",
    "\n",
    "Under a purely random policy, would you expect to hit this maximum often? Why or why not?\n",
    "\n",
    "Suppose each episode gives you a return R_1, R_2, ..., R_n under random actions.\n",
    "\n",
    "How would you compute the empirical average return and standard deviation of this random policy?\n",
    "\n",
    "Why is it useful to run a random baseline before training a learning agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777ed1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(episode_rewards, gamma):\n",
    "    \"\"\"\n",
    "    Computes discounted returns G_t without normalization.\n",
    "    \"\"\"\n",
    "    discounted = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        # TODO (Q2.1): implement the recursive discounted return\n",
    "        # cumulative = ...\n",
    "        # discounted[i] = ...\n",
    "        pass\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(episode_rewards, gamma):\n",
    "    discounted = discount_rewards(episode_rewards, gamma)\n",
    "    # TODO (Q2.2): normalize to mean 0 and std 1\n",
    "    # mean = ...\n",
    "    # std = ...\n",
    "    # return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435197c",
   "metadata": {},
   "source": [
    "**Q2.1**: (conceptual + coding)\n",
    "In discount_and_normalize_rewards we normalize the discounted returns:\n",
    "\n",
    "Complete the code that subtracts the mean and divides by the standard deviation (with small epsilon to avoid division by zero).\n",
    "\n",
    "Explain why normalization of returns can help training in vanilla policy gradient (REINFORCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccafafe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # TODO (Q3.1): define layers described below\n",
    "        # self.fc1 = ...\n",
    "        # self.fc2 = ...\n",
    "        # self.fc3 = ...\n",
    "\n",
    "        # TODO (Q3.2): apply Xavier initialization to all layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO (Q3.3): implement forward pass with ReLU activations on first two layers\n",
    "        # and raw logits from final layer\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752707b",
   "metadata": {},
   "source": [
    "**Q3.1** (coding)\n",
    "Implement a network with the following structure:\n",
    "\n",
    "fc1: state_size → 10 with ReLU\n",
    "\n",
    "fc2: 10 → action_size with ReLU\n",
    "\n",
    "fc3: action_size → action_size with no activation (logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce151f0",
   "metadata": {},
   "source": [
    "**Q3.2** (conceptual + coding)\n",
    "Use Xavier/Glorot initialization for each layer’s weight matrix.\n",
    "\n",
    "Add the initialization code.\n",
    "\n",
    "In your own words, what problem is weight initialization trying to avoid?\n",
    "\n",
    "Why is Xavier a reasonable choice for networks with ReLU or near-ReLU activations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db30531",
   "metadata": {},
   "source": [
    "**Q3.3** (conceptual)\n",
    "The network outputs raw logits, which you later pass through a softmax to get action probabilities.\n",
    "\n",
    "Why do we typically output logits rather than probabilities directly from the network?\n",
    "\n",
    "What is the relationship between logits and the softmax output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44357a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_random_agent(env, num_episodes=10, render=False):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"Random agent - EPISODE\", episode)\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Uniform random action (random guess)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(\"Score:\", total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(\"Random agent average reward over\", num_episodes, \"episodes:\", avg_reward)\n",
    "    return rewards\n",
    "\n",
    "# Run random agent baseline\n",
    "random_rewards = run_random_agent(env, num_episodes=10, render=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97330e51",
   "metadata": {},
   "source": [
    "**Part 4** — Vanilla Policy Gradient (REINFORCE)\n",
    "\n",
    "This is your train_policy_gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b554e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "policy_pg = PolicyNetwork(state_size, action_size)\n",
    "criterion_pg = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer_pg = optim.Adam(policy_pg.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_policy_gradient(env, num_episodes, render=False):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            logits = policy_pg(state_tensor)\n",
    "            # TODO (Q4.1): convert logits to action probabilities via softmax\n",
    "            # probs = ...\n",
    "\n",
    "            # TODO (Q4.2): sample an action from the probability distribution\n",
    "            # action = ...\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # TODO (Q4.3): compute discounted & normalized returns\n",
    "        # discounted_rewards = ...\n",
    "\n",
    "        # TODO (Q4.4): convert episode data to tensors\n",
    "        # states_tensor = ...\n",
    "        # actions_tensor = ...\n",
    "        # rewards_tensor = ...\n",
    "\n",
    "        # TODO (Q4.5): forward pass on all states, compute per-step negative log-prob\n",
    "        # logits = ...\n",
    "        # neg_log_prob = criterion_pg(...)\n",
    "\n",
    "        # TODO (Q4.6): compute policy gradient loss: mean(neg_log_prob * rewards_tensor)\n",
    "\n",
    "        # Backprop & optimizer step\n",
    "        optimizer_pg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pg.step()\n",
    "\n",
    "    return all_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e387bb2",
   "metadata": {},
   "source": [
    "**Q4.1** (coding)\n",
    "Given logits of shape [1, action_size], convert them to probabilities using softmax over the correct dimension.\n",
    "\n",
    "**Q4.2** (conceptual + coding)\n",
    "Sample an action according to the probabilities probs.\n",
    "\n",
    "Implement the sampling using  PyTorch.\n",
    "\n",
    "Why do we sample an action instead of always taking the argmax during training?\n",
    "\n",
    "**Q4.3** (coding)\n",
    "Use your discount_and_normalize_rewards function to compute discounted_rewards for the episode.\n",
    "\n",
    "**Q4.4** (coding)\n",
    "Convert episode_states, episode_actions, and discounted_rewards into proper PyTorch tensors:\n",
    "\n",
    "states_tensor: shape [T, state_size], dtype=torch.float32\n",
    "\n",
    "actions_tensor: shape [T], dtype=torch.long\n",
    "\n",
    "rewards_tensor: shape [T], dtype=torch.float32\n",
    "\n",
    "**Q4.5** (math + coding)\n",
    "We use criterion_pg = nn.CrossEntropyLoss(reduction='none'). For logits of shape [T, action_size] and actions_tensor of shape [T]:\n",
    "\n",
    "Implement the call to compute the negative log probability of the actions taken at each time step.\n",
    "\n",
    "**Q4.6** (math + coding)\n",
    "\n",
    "\n",
    "Implement loss = torch.mean(neg_log_prob * rewards_tensor).\n",
    "\n",
    "Explain why multiplying negative log-probabilities by discounted returns implements the REINFORCE gradient (in expectation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32b131",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10, render=False, title=\"Policy\"):\n",
    "    rewards = []\n",
    "    policy.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for episode in range(num_episodes):\n",
    "            state = reset_env(env)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            print(\"****************************************************\")\n",
    "            print(f\"{title} - EPISODE\", episode)\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                logits = policy(state_tensor)\n",
    "                probs = F.softmax(logits, dim=1).numpy().ravel()\n",
    "                action = np.random.choice(action_size, p=probs)\n",
    "\n",
    "                next_state, reward, done, _ = step_env(env, action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            print(\"Score:\", total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"{title} average score over {num_episodes} episodes:\", avg_reward)\n",
    "    return rewards\n",
    "\n",
    "# Load & evaluate vanilla PG model\n",
    "loaded_policy_pg = PolicyNetwork(state_size, action_size)\n",
    "loaded_policy_pg.load_state_dict(torch.load(MODEL_PATH_PG, map_location=torch.device('cpu')))\n",
    "\n",
    "pg_eval_rewards = evaluate_policy(env, loaded_policy_pg,\n",
    "                                  num_episodes=10,\n",
    "                                  render=False,\n",
    "                                  title=\"Vanilla PG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84501cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Policy gradient with baseline (learned state-value function)\n",
    "policy_pg_baseline = PolicyNetwork(state_size, action_size)\n",
    "value_baseline = ValueNetwork(state_size)\n",
    "\n",
    "# Single optimizer over both networks\n",
    "optimizer_pg_baseline = optim.Adam(\n",
    "    list(policy_pg_baseline.parameters()) + list(value_baseline.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "MODEL_PATH_PG_BASELINE = \"./models/cartpole_pg_baseline_pytorch.pth\"\n",
    "\n",
    "if TENSORBOARD_AVAILABLE:\n",
    "    writer_pg_baseline = SummaryWriter(log_dir=\"./runs/pg_baseline_cartpole_pytorch\")\n",
    "else:\n",
    "    writer_pg_baseline = None\n",
    "\n",
    "def train_policy_gradient_with_baseline(env,\n",
    "                                        num_episodes=max_episodes_pg_baseline,\n",
    "                                        render=False,\n",
    "                                        value_loss_coef=0.5):\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            logits = policy_pg_baseline(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1).detach().numpy().ravel()\n",
    "            action = np.random.choice(action_size, p=probs)\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Episode summary\n",
    "        episode_reward_sum = np.sum(episode_rewards)\n",
    "        all_rewards.append(episode_reward_sum)\n",
    "        mean_reward = np.mean(all_rewards)\n",
    "        max_reward = np.max(all_rewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"PG + Baseline - Episode:\", episode)\n",
    "        print(\"Reward:\", episode_reward_sum)\n",
    "        print(\"Mean reward:\", mean_reward)\n",
    "        print(\"Max reward so far:\", max_reward)\n",
    "\n",
    "        # Discounted returns (no normalization) for value network\n",
    "        discounted_returns = discount_rewards(episode_rewards, gamma)\n",
    "\n",
    "        # Tensors\n",
    "        states_tensor = torch.tensor(episode_states, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(episode_actions, dtype=torch.long)\n",
    "        returns_tensor = torch.tensor(discounted_returns, dtype=torch.float32)\n",
    "\n",
    "        # Forward\n",
    "        logits = policy_pg_baseline(states_tensor)\n",
    "        values = value_baseline(states_tensor)  # shape [T]\n",
    "\n",
    "        neg_log_prob = criterion_pg(logits, actions_tensor)  # -log π(a|s)\n",
    "\n",
    "        # Advantage = G_t - V(s_t); treat V(s) as baseline (no gradient in policy loss)\n",
    "        advantages = returns_tensor - values.detach()\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Policy loss (REINFORCE with baseline)\n",
    "        policy_loss = torch.mean(neg_log_prob * advantages)\n",
    "\n",
    "        # Value loss (fit V(s) to returns)\n",
    "        value_loss = F.mse_loss(values, returns_tensor)\n",
    "\n",
    "        # Total loss\n",
    "        loss = policy_loss + value_loss_coef * value_loss\n",
    "\n",
    "        # Optimize both networks\n",
    "        optimizer_pg_baseline.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pg_baseline.step()\n",
    "\n",
    "        # TensorBoard logging\n",
    "        if writer_pg_baseline is not None:\n",
    "            writer_pg_baseline.add_scalar(\"Loss/policy\", policy_loss.item(), episode)\n",
    "            writer_pg_baseline.add_scalar(\"Loss/value\", value_loss.item(), episode)\n",
    "            writer_pg_baseline.add_scalar(\"Reward_mean\", mean_reward, episode)\n",
    "\n",
    "        # Save periodically\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(policy_pg_baseline.state_dict(), MODEL_PATH_PG_BASELINE)\n",
    "            print(\"Saved PG+baseline model at episode\", episode)\n",
    "\n",
    "    # Final save\n",
    "    torch.save(policy_pg_baseline.state_dict(), MODEL_PATH_PG_BASELINE)\n",
    "    print(\"Final PG+baseline model saved.\")\n",
    "    if writer_pg_baseline is not None:\n",
    "        writer_pg_baseline.flush()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "# Train policy gradient with baseline\n",
    "pg_baseline_rewards = train_policy_gradient_with_baseline(env,\n",
    "                                                          num_episodes=max_episodes_pg_baseline,\n",
    "                                                          render=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293bbfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load & evaluate PG + baseline model\n",
    "loaded_policy_pg_baseline = PolicyNetwork(state_size, action_size)\n",
    "loaded_policy_pg_baseline.load_state_dict(\n",
    "    torch.load(MODEL_PATH_PG_BASELINE, map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "pg_baseline_eval_rewards = evaluate_policy(env,\n",
    "                                           loaded_policy_pg_baseline,\n",
    "                                           num_episodes=10,\n",
    "                                           render=False,\n",
    "                                           title=\"PG + Baseline\")\n",
    "\n",
    "# Optional: comparison summary (if you've run all cells)\n",
    "print(\"\\n=== Summary over 10 evaluation episodes (if all agents were run) ===\")\n",
    "if 'random_rewards' in globals():\n",
    "    print(\"Random agent average reward:      \", np.mean(random_rewards))\n",
    "else:\n",
    "    print(\"Random agent average reward:      (not run in this session)\")\n",
    "print(\"Vanilla PG average reward:       \", np.mean(pg_eval_rewards))\n",
    "print(\"PG with baseline average reward: \", np.mean(pg_baseline_eval_rewards))\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
