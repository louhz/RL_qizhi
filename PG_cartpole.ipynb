{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1436e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# TensorBoard support\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_AVAILABLE = False\n",
    "    SummaryWriter = None\n",
    "    print(\"TensorBoard not available. Scalar logging will be disabled.\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped  # same as your TF code\n",
    "\n",
    "# Seeding for reproducibility (handles both old & new gym APIs)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "try:\n",
    "    env.reset(seed=seed)\n",
    "except TypeError:\n",
    "    # Older gym versions:\n",
    "    env.seed(seed)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "max_episodes_pg = 300           # vanilla policy gradient\n",
    "max_episodes_pg_baseline = 300  # policy gradient with baseline\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95  # Discount rate\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba4a0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def reset_env(env):\n",
    "    \"\"\"\n",
    "    Handles both old and new Gym reset API:\n",
    "    - old: obs = env.reset()\n",
    "    - new: obs, info = env.reset()\n",
    "    \"\"\"\n",
    "    result = env.reset()\n",
    "    if isinstance(result, tuple):\n",
    "        state, _ = result\n",
    "    else:\n",
    "        state = result\n",
    "    return state\n",
    "\n",
    "def step_env(env, action):\n",
    "    \"\"\"\n",
    "    Handles both old and new Gym step API:\n",
    "    - old: obs, reward, done, info\n",
    "    - new: obs, reward, terminated, truncated, info\n",
    "    \"\"\"\n",
    "    result = env.step(action)\n",
    "    if len(result) == 5:\n",
    "        next_state, reward, terminated, truncated, info = result\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        next_state, reward, done, info = result\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "def discount_rewards(episode_rewards, gamma=gamma):\n",
    "    \"\"\"\n",
    "    Computes discounted returns without normalization.\n",
    "    Used for training the value function (baseline).\n",
    "    \"\"\"\n",
    "    discounted = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted[i] = cumulative\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(episode_rewards, gamma=gamma):\n",
    "    \"\"\"\n",
    "    Computes discounted returns and normalizes them.\n",
    "    Used by vanilla policy gradient (REINFORCE).\n",
    "    \"\"\"\n",
    "    discounted = discount_rewards(episode_rewards, gamma)\n",
    "    mean = np.mean(discounted)\n",
    "    std = np.std(discounted) + 1e-8  # avoid division by zero\n",
    "    return (discounted - mean) / std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccafafe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches your TF network structure:\n",
    "    fc1:  state_size -> 10 (ReLU)\n",
    "    fc2:  10 -> action_size (ReLU)\n",
    "    fc3:  action_size -> action_size (logits, no activation)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 10)\n",
    "        self.fc2 = nn.Linear(10, action_size)\n",
    "        self.fc3 = nn.Linear(action_size, action_size)\n",
    "        \n",
    "        # Xavier initialization similar to tf.contrib.layers.xavier_initializer()\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits  # raw logits\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple state-value function V(s) used as a learned baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        value = self.fc2(x)  # shape [batch, 1]\n",
    "        return value.squeeze(-1)  # shape [batch]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44357a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_random_agent(env, num_episodes=10, render=False):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"Random agent - EPISODE\", episode)\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Uniform random action (random guess)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(\"Score:\", total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(\"Random agent average reward over\", num_episodes, \"episodes:\", avg_reward)\n",
    "    return rewards\n",
    "\n",
    "# Run random agent baseline\n",
    "random_rewards = run_random_agent(env, num_episodes=10, render=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b554e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vanilla policy gradient (REINFORCE) setup\n",
    "policy_pg = PolicyNetwork(state_size, action_size)\n",
    "criterion_pg = nn.CrossEntropyLoss(reduction='none')  # per-sample negative log-prob\n",
    "optimizer_pg = optim.Adam(policy_pg.parameters(), lr=learning_rate)\n",
    "\n",
    "MODEL_PATH_PG = \"./models/cartpole_pg_pytorch.pth\"\n",
    "\n",
    "if TENSORBOARD_AVAILABLE:\n",
    "    writer_pg = SummaryWriter(log_dir=\"./runs/pg_cartpole_pytorch\")\n",
    "else:\n",
    "    writer_pg = None\n",
    "\n",
    "def train_policy_gradient(env, num_episodes=max_episodes_pg, render=False):\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            logits = policy_pg(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1).detach().numpy().ravel()\n",
    "            action = np.random.choice(action_size, p=probs)\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Episode summary\n",
    "        episode_reward_sum = np.sum(episode_rewards)\n",
    "        all_rewards.append(episode_reward_sum)\n",
    "        mean_reward = np.mean(all_rewards)\n",
    "        max_reward = np.max(all_rewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Vanilla PG - Episode:\", episode)\n",
    "        print(\"Reward:\", episode_reward_sum)\n",
    "        print(\"Mean reward:\", mean_reward)\n",
    "        print(\"Max reward so far:\", max_reward)\n",
    "\n",
    "        # Discount and normalize rewards\n",
    "        discounted_rewards = discount_and_normalize_rewards(episode_rewards, gamma)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_tensor = torch.tensor(episode_states, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(episode_actions, dtype=torch.long)\n",
    "        rewards_tensor = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = policy_pg(states_tensor)\n",
    "        neg_log_prob = criterion_pg(logits, actions_tensor)  # shape [T]\n",
    "\n",
    "        # Policy gradient loss\n",
    "        loss = torch.mean(neg_log_prob * rewards_tensor)\n",
    "\n",
    "        # Backprop and update\n",
    "        optimizer_pg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pg.step()\n",
    "\n",
    "        # TensorBoard logging\n",
    "        if writer_pg is not None:\n",
    "            writer_pg.add_scalar(\"Loss\", loss.item(), episode)\n",
    "            writer_pg.add_scalar(\"Reward_mean\", mean_reward, episode)\n",
    "\n",
    "        # Save model periodically\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(policy_pg.state_dict(), MODEL_PATH_PG)\n",
    "            print(\"Saved vanilla PG model at episode\", episode)\n",
    "\n",
    "    # Final save\n",
    "    torch.save(policy_pg.state_dict(), MODEL_PATH_PG)\n",
    "    print(\"Final vanilla PG model saved.\")\n",
    "    if writer_pg is not None:\n",
    "        writer_pg.flush()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "# Train vanilla policy gradient\n",
    "pg_rewards = train_policy_gradient(env, num_episodes=max_episodes_pg, render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32b131",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10, render=False, title=\"Policy\"):\n",
    "    rewards = []\n",
    "    policy.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for episode in range(num_episodes):\n",
    "            state = reset_env(env)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            print(\"****************************************************\")\n",
    "            print(f\"{title} - EPISODE\", episode)\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                logits = policy(state_tensor)\n",
    "                probs = F.softmax(logits, dim=1).numpy().ravel()\n",
    "                action = np.random.choice(action_size, p=probs)\n",
    "\n",
    "                next_state, reward, done, _ = step_env(env, action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            print(\"Score:\", total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"{title} average score over {num_episodes} episodes:\", avg_reward)\n",
    "    return rewards\n",
    "\n",
    "# Load & evaluate vanilla PG model\n",
    "loaded_policy_pg = PolicyNetwork(state_size, action_size)\n",
    "loaded_policy_pg.load_state_dict(torch.load(MODEL_PATH_PG, map_location=torch.device('cpu')))\n",
    "\n",
    "pg_eval_rewards = evaluate_policy(env, loaded_policy_pg,\n",
    "                                  num_episodes=10,\n",
    "                                  render=False,\n",
    "                                  title=\"Vanilla PG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84501cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Policy gradient with baseline (learned state-value function)\n",
    "policy_pg_baseline = PolicyNetwork(state_size, action_size)\n",
    "value_baseline = ValueNetwork(state_size)\n",
    "\n",
    "# Single optimizer over both networks\n",
    "optimizer_pg_baseline = optim.Adam(\n",
    "    list(policy_pg_baseline.parameters()) + list(value_baseline.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "MODEL_PATH_PG_BASELINE = \"./models/cartpole_pg_baseline_pytorch.pth\"\n",
    "\n",
    "if TENSORBOARD_AVAILABLE:\n",
    "    writer_pg_baseline = SummaryWriter(log_dir=\"./runs/pg_baseline_cartpole_pytorch\")\n",
    "else:\n",
    "    writer_pg_baseline = None\n",
    "\n",
    "def train_policy_gradient_with_baseline(env,\n",
    "                                        num_episodes=max_episodes_pg_baseline,\n",
    "                                        render=False,\n",
    "                                        value_loss_coef=0.5):\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = reset_env(env)\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            logits = policy_pg_baseline(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1).detach().numpy().ravel()\n",
    "            action = np.random.choice(action_size, p=probs)\n",
    "\n",
    "            next_state, reward, done, _ = step_env(env, action)\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Episode summary\n",
    "        episode_reward_sum = np.sum(episode_rewards)\n",
    "        all_rewards.append(episode_reward_sum)\n",
    "        mean_reward = np.mean(all_rewards)\n",
    "        max_reward = np.max(all_rewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"PG + Baseline - Episode:\", episode)\n",
    "        print(\"Reward:\", episode_reward_sum)\n",
    "        print(\"Mean reward:\", mean_reward)\n",
    "        print(\"Max reward so far:\", max_reward)\n",
    "\n",
    "        # Discounted returns (no normalization) for value network\n",
    "        discounted_returns = discount_rewards(episode_rewards, gamma)\n",
    "\n",
    "        # Tensors\n",
    "        states_tensor = torch.tensor(episode_states, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(episode_actions, dtype=torch.long)\n",
    "        returns_tensor = torch.tensor(discounted_returns, dtype=torch.float32)\n",
    "\n",
    "        # Forward\n",
    "        logits = policy_pg_baseline(states_tensor)\n",
    "        values = value_baseline(states_tensor)  # shape [T]\n",
    "\n",
    "        neg_log_prob = criterion_pg(logits, actions_tensor)  # -log Ï€(a|s)\n",
    "\n",
    "        # Advantage = G_t - V(s_t); treat V(s) as baseline (no gradient in policy loss)\n",
    "        advantages = returns_tensor - values.detach()\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Policy loss (REINFORCE with baseline)\n",
    "        policy_loss = torch.mean(neg_log_prob * advantages)\n",
    "\n",
    "        # Value loss (fit V(s) to returns)\n",
    "        value_loss = F.mse_loss(values, returns_tensor)\n",
    "\n",
    "        # Total loss\n",
    "        loss = policy_loss + value_loss_coef * value_loss\n",
    "\n",
    "        # Optimize both networks\n",
    "        optimizer_pg_baseline.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pg_baseline.step()\n",
    "\n",
    "        # TensorBoard logging\n",
    "        if writer_pg_baseline is not None:\n",
    "            writer_pg_baseline.add_scalar(\"Loss/policy\", policy_loss.item(), episode)\n",
    "            writer_pg_baseline.add_scalar(\"Loss/value\", value_loss.item(), episode)\n",
    "            writer_pg_baseline.add_scalar(\"Reward_mean\", mean_reward, episode)\n",
    "\n",
    "        # Save periodically\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(policy_pg_baseline.state_dict(), MODEL_PATH_PG_BASELINE)\n",
    "            print(\"Saved PG+baseline model at episode\", episode)\n",
    "\n",
    "    # Final save\n",
    "    torch.save(policy_pg_baseline.state_dict(), MODEL_PATH_PG_BASELINE)\n",
    "    print(\"Final PG+baseline model saved.\")\n",
    "    if writer_pg_baseline is not None:\n",
    "        writer_pg_baseline.flush()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "# Train policy gradient with baseline\n",
    "pg_baseline_rewards = train_policy_gradient_with_baseline(env,\n",
    "                                                          num_episodes=max_episodes_pg_baseline,\n",
    "                                                          render=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293bbfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load & evaluate PG + baseline model\n",
    "loaded_policy_pg_baseline = PolicyNetwork(state_size, action_size)\n",
    "loaded_policy_pg_baseline.load_state_dict(\n",
    "    torch.load(MODEL_PATH_PG_BASELINE, map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "pg_baseline_eval_rewards = evaluate_policy(env,\n",
    "                                           loaded_policy_pg_baseline,\n",
    "                                           num_episodes=10,\n",
    "                                           render=False,\n",
    "                                           title=\"PG + Baseline\")\n",
    "\n",
    "# Optional: comparison summary (if you've run all cells)\n",
    "print(\"\\n=== Summary over 10 evaluation episodes (if all agents were run) ===\")\n",
    "if 'random_rewards' in globals():\n",
    "    print(\"Random agent average reward:      \", np.mean(random_rewards))\n",
    "else:\n",
    "    print(\"Random agent average reward:      (not run in this session)\")\n",
    "print(\"Vanilla PG average reward:       \", np.mean(pg_eval_rewards))\n",
    "print(\"PG with baseline average reward: \", np.mean(pg_baseline_eval_rewards))\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
